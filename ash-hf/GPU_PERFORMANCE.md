# GPU Performance Comparison

## GPU Options on Modal

| GPU | VRAM | Performance | Cost/hr | Best For |
|-----|------|-------------|---------|----------|
| **A100** | 40 GB | 100% (baseline) | ~$4.00 | **Training (FASTEST)** âœ… |
| A10G | 24 GB | ~33% | ~$1.20 | Budget training |
| T4 | 16 GB | ~20% | ~$0.60 | Very slow, not recommended |
| L4 | 24 GB | ~40% | ~$1.50 | Good middle ground |

## Your Current Setup

**OLD (A10G):**
- GPU: A10G (24 GB VRAM)
- Speed: 1x baseline
- Cost: ~$1.20/hr
- Games: ~3-6 seconds each

**NEW (A100):** âš¡
- GPU: A100 (40 GB VRAM)
- Speed: **3x faster** than A10G
- Cost: ~$4.00/hr
- Games: **~1-2 seconds each**
- Batch size: 128 (vs 64)
- More CPU cores: 4 (vs 2)

## Performance Improvements

### Speed Comparison (500 iterations)

| Config | GPU | Time | Cost | Games/sec |
|--------|-----|------|------|-----------|
| Old | A10G | 30-40 hrs | $36-48 | 0.2-0.3 |
| **New** | **A100** | **10-15 hrs** | **$40-60** | **0.6-1.0** |

### Why A100 is Better for Training

âœ… **3x faster compute** - Training epochs complete faster
âœ… **2x memory bandwidth** - Faster data loading
âœ… **Larger batch sizes** - 128 vs 64 (better gradient estimates)
âœ… **TensorFloat32 support** - Hardware-accelerated matrix ops
âœ… **Better FP16 performance** - Mixed precision training is faster

## Cost Analysis

### Old Setup (A10G):
- 500 iterations Ã— 20 games Ã— 30 simulations
- Time: 35 hours
- Cost: 35 hrs Ã— $1.20/hr = **$42**

### New Setup (A100):
- 500 iterations Ã— 20 games Ã— 30 simulations
- Time: **12 hours** (3x faster)
- Cost: 12 hrs Ã— $4.00/hr = **$48**

**Result:** Only $6 more expensive, but **3x faster!** ðŸš€

## Optimizations Applied

1. âœ… **GPU: A100** (was A10G)
2. âœ… **Batch size: 128** (was 64)
3. âœ… **CPU cores: 4** (was 2)
4. âœ… **Checkpoint frequency: 10** (was 5) - less I/O overhead
5. âœ… **Timeout: 48 hours** (was 24) - supports longer runs

## Expected Training Times (A100)

| Iterations | Games | Simulations | Time | Cost | Elo |
|-----------|-------|-------------|------|------|-----|
| 100 | 20 | 30 | 1-2 hrs | $4-8 | 1000-1200 |
| 200 | 20 | 30 | 2-4 hrs | $8-16 | 1200-1400 |
| 500 | 20 | 30 | 10-15 hrs | $40-60 | 1400-1800 |
| 1000 | 25 | 40 | 25-35 hrs | $100-140 | 1800-2200 |

## When to Use Each GPU

### Use A100 (Current) âœ…
- **Training** - Fastest training speed
- Long runs (500+ iterations)
- You want results quickly
- Budget: $40-100

### Use A10G (Budget)
- Budget training (100-200 iterations)
- You're not in a hurry
- Cost conscious (<$30)
- Testing/development

### Use L4 (Middle Ground)
- 200-300 iterations
- Balance of speed and cost
- Medium budget ($20-50)

## How to Switch GPUs

Edit [train_continuous_modal.py:30](c:\Users\ianto\chesshacks_training\ash-hf\src\train_continuous_modal.py#L30):

```python
# A100 (Fastest) - Current setting
gpu="A100"

# A10G (Budget)
gpu="A10G"

# L4 (Middle ground)
gpu="L4"

# T4 (Very slow, not recommended)
gpu="T4"
```

## Recommendations

### For ChessHacks Competition:
**Use A100** - Get a strong model quickly (1400-1800 Elo in 12-15 hours for $40-60)

### For Budget Training:
**Use A10G** - Slower but cheaper (1200-1600 Elo in 30-40 hours for $36-48)

### For Experimentation:
**Use A10G or L4** - Test configurations without spending too much

## Running with A100

The same command works with the faster GPU:

```batch
cd ash-hf\src
modal run train_continuous_modal.py --iterations 500 --games 20 --simulations 30
```

Now it will:
- Use A100 GPU (3x faster)
- Complete in ~12 hours (vs 35 hours)
- Cost ~$48 (vs $42, only $6 more)
- Give you a 1400-1800 Elo bot

**Worth the extra $6 to save 23 hours!** âš¡
